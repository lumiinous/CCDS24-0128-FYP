{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7986a-df5b-4049-b894-664697a4ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export LD_LIBRARY_PATH=/openfhe-python/openfhe/:$LD_LIBRARY_PATH\n",
    "# !export PYTHONPATH=/openfhe-python/openfhe/:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0686fb-dc41-43c7-898b-ff131aeb62c9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fabaca-0b2f-4786-a463-b1fd8d842477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/openfhe-python/openfhe/')\n",
    "from openfhe import *\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3e62e-5615-4cb3-85c2-d33b77e9cf6e",
   "metadata": {},
   "source": [
    "# Setup openFHE context, generate keys, encrypt and decrypt functions then test them against a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521d2af-fd56-492c-b636-4f2f8f5e45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Setup OpenFHE CKKS context and generate keys\n",
    "def setup_openfhe():\n",
    "    params = CCParamsCKKSRNS()\n",
    "    params.SetScalingModSize(40)  # Set the scaling factor for CKKS\n",
    "    params.SetMultiplicativeDepth(9)  # Set the multiplicative depth\n",
    "    params.SetSecurityLevel(HEStd_NotSet)\n",
    "    params.SetRingDim(32768)\n",
    "    params.SetScalingTechnique(FIXEDAUTO)\n",
    "    params.SetBatchSize(16)\n",
    "    \n",
    "    cc = GenCryptoContext(params)  # Generate the CKKS context\n",
    "    cc.Enable(PKESchemeFeature.PKE)  # Enable public key encryption features\n",
    "    cc.Enable(PKESchemeFeature.KEYSWITCH)  # Enable key switching\n",
    "    cc.Enable(PKESchemeFeature.LEVELEDSHE)  # Enable leveled homomorphic encryption\n",
    "    cc.Enable(PKESchemeFeature.ADVANCEDSHE)  # Enable leveled homomorphic encryption\n",
    "    cc.Enable (PKESchemeFeature.MULTIPARTY)\n",
    "    print(\"Mod size: \",params.GetFirstModSize())\n",
    "    print(\"Ring Dim: \",params.GetRingDim())\n",
    "    return cc\n",
    "\n",
    "# Step 2: Encrypt input using CKKS\n",
    "def encrypt_input(cc, public_key, input_data):\n",
    "    # Create a CKKS packed plaintext from the input data\n",
    "    #print(\"Encrypting...\")\n",
    "    plaintext = cc.MakeCKKSPackedPlaintext(input_data)\n",
    "    \n",
    "    # Encrypt the plaintext using the public key\n",
    "    ciphertext = cc.Encrypt(public_key, plaintext)\n",
    "    #print(\"ciphertext:\"+ str(ciphertext))\n",
    "    return ciphertext\n",
    "\n",
    "# Step 3: Decrypt ciphertext back to plaintext (with trimming and handling small imaginary parts)\n",
    "def decrypt_input(cc, kp1_secret_key, kp2_secret_key, ciphertext, original_size):\n",
    "    ciphertextPartial1 = cc.MultipartyDecryptLead([ciphertext], kp1_secret_key)\n",
    "    ciphertextPartial2 = cc.MultipartyDecryptMain([ciphertext], kp2_secret_key)\n",
    "    partialCiphertextVec = [ciphertextPartial1[0], ciphertextPartial2[0]]\n",
    "    decrypted_plaintext = cc.MultipartyDecryptFusion(partialCiphertextVec)\n",
    "    decrypted_values = decrypted_plaintext.GetCKKSPackedValue()\n",
    "    trimmed_decrypted_values = decrypted_values[:original_size]\n",
    "    real_decrypted_values = [np.real_if_close(value, tol=1e-15) for value in trimmed_decrypted_values]\n",
    "    float_decrypted_values = [float(np.real(value)) for value in real_decrypted_values]\n",
    "    return float_decrypted_values\n",
    "\n",
    "#key generation\n",
    "cc=setup_openfhe()\n",
    "\n",
    "# For Party A\n",
    "kp1 = cc.KeyGen()\n",
    "evalMultKey = cc.KeySwitchGen(kp1.secretKey, kp1.secretKey)\n",
    "cc.EvalSumKeyGen(kp1.secretKey)\n",
    "evalSumKeys = cc.GetEvalSumKeyMap(kp1.secretKey.GetKeyTag())\n",
    "cc.EvalAtIndexKeyGen(kp1.secretKey, [-1])\n",
    "evalAtIndexKeys=cc.GetEvalAutomorphismKeyMap(kp1.secretKey.GetKeyTag())\n",
    "\n",
    "# For Party B\n",
    "kp2 = cc.MultipartyKeyGen(kp1.publicKey)\n",
    "evalMultKey2 = cc.MultiKeySwitchGen(kp2.secretKey, kp2.secretKey, evalMultKey)\n",
    "\n",
    "# Combine evaluation multiplication keys from both parties\n",
    "evalMultAB = cc.MultiAddEvalKeys(evalMultKey, evalMultKey2, kp2.publicKey.GetKeyTag())\n",
    "evalMultBAB = cc.MultiMultEvalKey(kp2.secretKey, evalMultAB, kp2.publicKey.GetKeyTag())\n",
    "\n",
    "# Summation keys for both parties\n",
    "evalSumKeysB = cc.MultiEvalSumKeyGen(kp2.secretKey, evalSumKeys, kp2.publicKey.GetKeyTag())\n",
    "evalSumKeysJoin = cc.MultiAddEvalSumKeys(evalSumKeys, evalSumKeysB, kp2.publicKey.GetKeyTag())\n",
    "cc.InsertEvalSumKey(evalSumKeysJoin)\n",
    "\n",
    "# Final evaluation multiplication keys from both parties\n",
    "evalMultAAB = cc.MultiMultEvalKey(kp1.secretKey, evalMultAB, kp2.publicKey.GetKeyTag())\n",
    "evalMultFinal = cc.MultiAddEvalMultKeys(evalMultAAB, evalMultBAB, evalMultAB.GetKeyTag())\n",
    "\n",
    "# Insert the final multiplication key\n",
    "cc.InsertEvalMultKey([evalMultFinal])\n",
    "evalAtIndexKeysB=cc.MultiEvalAtIndexKeyGen(kp2.secretKey, evalAtIndexKeys, [-1], kp2.publicKey.GetKeyTag())\n",
    "evalAtIndexKeysJoin = cc.MultiAddEvalAutomorphismKeys(evalAtIndexKeys, evalAtIndexKeysB, kp2.publicKey.GetKeyTag())\n",
    "cc.InsertEvalSumKey(evalAtIndexKeysJoin)#its the same function?\n",
    "\n",
    "# Define two-party bootstrapping function\n",
    "def two_party_bootstrapping(ciphertext, partyA, partyB, cc):\n",
    "    # Prepare ciphertext for bootstrapping by stripping c0\n",
    "    #print(\"Depth:\", ciphertext.GetLevel())\n",
    "    if ciphertext.GetLevel() < 2:\n",
    "        return ciphertext\n",
    "    c1 = ciphertext.Clone()\n",
    "    c1.RemoveElement(0)\n",
    "    \n",
    "    # Generate a common random polynomial (a)\n",
    "    a = cc.IntMPBootRandomElementGen(partyB.publicKey)\n",
    "    \n",
    "    # Each party generates its own decryption share\n",
    "    share1 = cc.IntMPBootDecrypt(partyA.secretKey, c1, a)\n",
    "    share2 = cc.IntMPBootDecrypt(partyB.secretKey, c1, a)\n",
    "    \n",
    "    # Aggregate the decryption shares\n",
    "    aggregatedShares = cc.IntMPBootAdd([share1, share2])\n",
    "    \n",
    "    # Re-encrypt (bootstrap) the ciphertext\n",
    "    bootstrapped_ciphertext = cc.IntMPBootEncrypt(partyB.publicKey, aggregatedShares, a, ciphertext)\n",
    "    \n",
    "    return bootstrapped_ciphertext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1597e-c46d-486a-bc82-7cf6f503d1e5",
   "metadata": {},
   "source": [
    "# Define dataset and train/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17633c-5706-4bea-b2a0-22d519e2572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the manual seed for reproducibility\n",
    "torch.random.manual_seed(73)\n",
    "random.seed(73)\n",
    "\n",
    "def split_train_test(x, y, test_ratio=0.3):\n",
    "    idxs = [i for i in range(len(x))]\n",
    "    random.shuffle(idxs)\n",
    "    # delimiter between test and train data\n",
    "    delim = int(len(x) * test_ratio)\n",
    "    test_idxs, train_idxs = idxs[:delim], idxs[delim:]\n",
    "    # Use indexing on tensors to split the data\n",
    "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]\n",
    "\n",
    "\n",
    "def heart_disease_data():\n",
    "    data = pd.read_csv(\"./framingham.csv\")\n",
    "    # drop rows with missing values\n",
    "    data = data.dropna()\n",
    "    # drop some features\n",
    "    #data = data.drop(columns=[\"education\", \"currentSmoker\", \"BPMeds\", \"diabetes\", \"diaBP\", \"BMI\"])\n",
    "    # balance data\n",
    "    grouped = data.groupby('TenYearCHD')\n",
    "    data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=73).reset_index(drop=True))\n",
    "    # extract labels\n",
    "    y = torch.tensor(data[\"TenYearCHD\"].values).float().unsqueeze(1)\n",
    "    data = data.drop(\"TenYearCHD\", axis=1)\n",
    "    # standardize data\n",
    "    data = (data - data.mean()) / data.std()\n",
    "    x = torch.tensor(data.values).float()\n",
    "    return split_train_test(x, y)\n",
    "\n",
    "\n",
    "def random_data(m=1024, n=2):\n",
    "    # data separable by the line `y = x`\n",
    "    x_train = torch.randn(m, n)\n",
    "    x_test = torch.randn(m // 2, n)\n",
    "    y_train = (x_train[:, 0] >= x_train[:, 1]).float().unsqueeze(0).t()\n",
    "    y_test = (x_test[:, 0] >= x_test[:, 1]).float().unsqueeze(0).t()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# You can use whatever data you want without modification to the tutorial\n",
    "# x_train, y_train, x_test, y_test = random_data()\n",
    "x_train, y_train, x_test, y_test = heart_disease_data()\n",
    "\n",
    "print(\"############# Data summary #############\")\n",
    "print(f\"x_train has shape: {x_train.shape}\")\n",
    "print(f\"y_train has shape: {y_train.shape}\")\n",
    "print(f\"x_test has shape: {x_test.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")\n",
    "print(\"#######################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397cf33-5cf8-4898-aa60-a55ecbc71239",
   "metadata": {},
   "source": [
    "# Define normal linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb107b-0b55-41f7-b119-c69df6182d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LR, self).__init__()\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "        #print(f\"Initialized bias: {self.lr.bias.data}\")\n",
    "    def forward(self, x):\n",
    "        #print(f\"Current bias: {self.lr.bias.data}\")\n",
    "        out= self.lr(x)\n",
    "        #print(\"Normal model before sigmoid: \"+ str(out))\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "n_features = x_train.shape[1]\n",
    "model = LR(n_features)\n",
    "# use gradient descent with a learning_rate=1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# use Binary Cross Entropy Loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "summary(model, input_size=(1, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069b668-24d7-4823-838e-70dd3f29ea3c",
   "metadata": {},
   "source": [
    "# Train normal linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f175d-b85b-4626-a8fd-66132cf35a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, optim, criterion, x, y, epochs=EPOCHS):\n",
    "    start_time = time.time()  # Record start time\n",
    "    loss_history = []  # Store loss values\n",
    "    loss_rate = []  # Store rate of loss decrease\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        optim.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)  # Store loss\n",
    "        \n",
    "        # Calculate rate of loss decrease (except for first epoch)\n",
    "        if e > 1:\n",
    "            rate = loss_history[-2] - loss_value\n",
    "            loss_rate.append(rate)\n",
    "\n",
    "        #print(f\"Loss at epoch {e}: {loss_value}\")\n",
    "\n",
    "    end_time = time.time()  # Record end time\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time taken for training: {total_time:.2f} seconds\")\n",
    "\n",
    "    # Plot the loss over epochs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Loss over epochs\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), loss_history, marker='o', linestyle='-', label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Rate of loss decrease\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(2, epochs + 1), loss_rate, marker='o', linestyle='-', color='r', label=\"Rate of Loss Decrease\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Rate of Loss Decrease\")\n",
    "    plt.title(\"Rate of Loss Decrease Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run training and visualize loss trends\n",
    "LR_Model = train(model, optim, criterion, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92580c2-27b2-4b0e-b877-270d8360622c",
   "metadata": {},
   "source": [
    "# Get accuracy of normal linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899fa4f-d24d-40f8-9983-57c301dfb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def accuracy(model, x, y):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Compute the model output\n",
    "    out = model(x)  # Ensure to use the input model parameter here\n",
    "    \n",
    "    # Apply threshold for binary classification\n",
    "    predictions = (out >= 0.5).float()  # Convert to 1 if out >= 0.5, else 0\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predictions == y).float()\n",
    "    accuracy = correct.mean()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total time taken for testing: {total_time:.2f} seconds\")\n",
    "    print(f\"Accuracy on plain test_set: {accuracy:.4f}\")\n",
    "    \n",
    "    # Plot the distribution of the outputs\n",
    "    out_values = out.detach().numpy()  # Convert to numpy for plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(out_values, bins=30, color='blue', edgecolor='black')\n",
    "    plt.xlabel(\"Output Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Model Outputs\")\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Call the function with model, test inputs, and test labels\n",
    "plain_accuracy = accuracy(LR_Model, x_test, y_test)\n",
    "\n",
    "\n",
    "# plain_accuracy = accuracy(model, x_train, y_train)\n",
    "# print(f\"Accuracy on plain train_set: {plain_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10384681-751c-4775-b5a2-13eb5711106b",
   "metadata": {},
   "source": [
    "# Define post training encrypted linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458d34f-1ac6-42e4-8031-4da00d0f2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR1:\n",
    "    def __init__(self, torch_lr, cc, public_key):\n",
    "        start_time = time.time()\n",
    "        # Extract weights and biases from the PyTorch model\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]  # Extract the weights from the Linear layer\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()  # Extract the biases from the Linear layer\n",
    "        \n",
    "        # Encrypt the weights and biases (done once during initialization)\n",
    "        self.weight_enc = cc.Encrypt(public_key, cc.MakeCKKSPackedPlaintext(self.weight))\n",
    "        self.bias_enc = cc.Encrypt(public_key, cc.MakeCKKSPackedPlaintext(self.bias))\n",
    "        self.cc = cc\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Total time taken for encrypting parameters: {total_time:.2f} seconds\")\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        # Homomorphically multiply encrypted inputs with encrypted weights using the CryptoContext\n",
    "        enc_out = self.cc.EvalInnerProduct(enc_x , self.weight_enc,15)\n",
    "        enc_out = self.cc.EvalAdd(enc_out, self.bias_enc)  # Add the encrypted bias\n",
    "\n",
    "        return enc_out\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "eelr1 = EncryptedLR1(LR_Model,cc,kp2.publicKey)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40a40-03e4-4c54-bd46-47870ed3e588",
   "metadata": {},
   "source": [
    "# Encrypt test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3c6f6-c30d-46ff-8308-898418d778da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "t_start = time.time()\n",
    "\n",
    "# Encrypt the test set\n",
    "enc_x_test = [encrypt_input(cc, kp2.publicKey, x.tolist()) for x in x_test]\n",
    "\n",
    "# End the timer\n",
    "t_end = time.time()\n",
    "\n",
    "# Print the time taken with more precision\n",
    "print(f\"Encryption of the test set took {t_end - t_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc08199-7351-46a7-b4f6-be70dae43716",
   "metadata": {},
   "source": [
    "# Get accuracy of post training encrypted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aacf18-66ee-4682-ac57-dd165434ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def encrypted_evaluation1(model, enc_x_test, y_test):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    correct = 0\n",
    "    outputs = []  # List to store the outputs for plotting\n",
    "    \n",
    "    for enc_x, y in zip(enc_x_test, y_test):\n",
    "        # Encrypted evaluation\n",
    "        enc_out = model(enc_x)\n",
    "        \n",
    "        # Decrypt the encrypted output\n",
    "        out = decrypt_input(cc, kp1.secretKey, kp2.secretKey, enc_out, 1)\n",
    "        out = torch.tensor(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        # Apply threshold to make binary prediction\n",
    "        predicted_label = 1 if out >= 0.5 else 0\n",
    "        \n",
    "        # Store the output for plotting (before thresholding)\n",
    "        outputs.append(out.item())\n",
    "        \n",
    "        # Check if the binary prediction matches the true label\n",
    "        if predicted_label == y.item():  # Ensure both are scalars\n",
    "            correct += 1\n",
    "    \n",
    "    t_end = time.time()\n",
    "    total_time = t_end - t_start\n",
    "    print(f\"Evaluated test set of {len(enc_x_test)} entries in {total_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {correct}/{len(enc_x_test)} = {correct / len(enc_x_test)}\")\n",
    "    \n",
    "    # Plot the distribution of model outputs before thresholding\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(outputs, bins=30, color='blue', edgecolor='black')\n",
    "    plt.xlabel(\"Output Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Model Outputs (Before Thresholding)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return correct / len(enc_x_test)\n",
    "\n",
    "# Perform encrypted evaluation and calculate difference with plain accuracy\n",
    "encrypted_accuracy = encrypted_evaluation1(eelr1, enc_x_test, y_test)\n",
    "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a4f37-b78e-4270-8c07-c2361dcad103",
   "metadata": {},
   "source": [
    "# Check the difference in output between normal model and FHE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4d880-157c-4eb0-ae0e-1519d854377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2 # For example, take the 6th input (indexing starts at 0)\n",
    "single_input = x_test[index]\n",
    "single_label = y_test[index]\n",
    "\n",
    "# Encrypt\n",
    "enc_single_input = enc_x_test[index]\n",
    "enc_single_input_decrypted = decrypt_input(cc, kp1.secretKey, kp2.secretKey, enc_single_input, 15)\n",
    "enc_single_label = y_test[index]\n",
    "\n",
    "# Pass to model\n",
    "output = LR_Model(single_input)\n",
    "enc_output = eelr1(enc_single_input)\n",
    "enc_output = decrypt_input(cc, kp1.secretKey, kp2.secretKey, enc_output, 1)\n",
    "enc_output = torch.sigmoid(torch.tensor(enc_output ))\n",
    "# Print\n",
    "print(f\"Input at index {index}: {single_input}\")\n",
    "print(f\"True Label at index {index}: {single_label}\")\n",
    "print(f\"Model Output: {output}\")\n",
    "\n",
    "print(\"===Encrypted===\")\n",
    "print(f\"Input at index {index}: {enc_single_input_decrypted}\")\n",
    "print(f\"True Label at index {index}: {enc_single_label}\")\n",
    "print(f\"Model Output: {enc_output }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b97af3-2695-44c9-b79a-ad9dd4f970d1",
   "metadata": {},
   "source": [
    "# Training on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916304e9-151d-47c2-830e-31cebc4b2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR2:\n",
    "    \n",
    "    def __init__(self,cc,public_key,torch_lr):\n",
    "        self.cc = cc \n",
    "        self.weight = cc.MakeCKKSPackedPlaintext(torch_lr.lr.weight.data.tolist()[0])\n",
    "        self.bias = cc.MakeCKKSPackedPlaintext(torch_lr.lr.bias.data.tolist())\n",
    "        print(self.weight)\n",
    "        print(self.bias)\n",
    "        self.weight_enc = cc.Encrypt(public_key, self.weight)\n",
    "        self.bias_enc = cc.Encrypt(public_key, self.bias)\n",
    "        self.zero1 = cc.Encrypt(public_key, cc.MakeCKKSPackedPlaintext([0]))  # Encrypted gradient accumulator\n",
    "        self.zero9 = cc.Encrypt(public_key, cc.MakeCKKSPackedPlaintext([0]*15))  # Encrypted gradient accumulator\n",
    "        self._delta_w = self.zero9 # Encrypted gradient accumulator\n",
    "        self._delta_b = self.zero1  # Encrypted bias accumulator\n",
    "        self._count = 0\n",
    "        self.key=public_key\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        # Compute encrypted inner product between inputs and weights\n",
    "        self.cc.RescaleInPlace(self.weight_enc)\n",
    "        enc_out = self.cc.EvalInnerProduct(enc_x, self.weight_enc, 15)\n",
    "        self.cc.RescaleInPlace(enc_out)\n",
    "        # Perform two-party bootstrapping to refresh ciphertext\n",
    "        enc_out = two_party_bootstrapping(enc_out, kp1, kp2, cc)\n",
    "        enc_out = self.cc.EvalAdd(enc_out, self.bias_enc)\n",
    "        # Apply the sigmoid approximation for activation\n",
    "        enc_out = self.sigmoid(enc_out)\n",
    "\n",
    "        return enc_out\n",
    "    \n",
    "    def backward(self,enc_x, enc_out, enc_y):\n",
    "        ext_out_minus_y = cc.Encrypt(self.key,self.cc.MakeCKKSPackedPlaintext([0] * 15))\n",
    "        mask=self.cc.MakeCKKSPackedPlaintext([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "        out_minus_y_w= self.cc.EvalSub(enc_out,enc_y)\n",
    "        out_minus_y_w= self.cc.EvalMult(out_minus_y_w,mask)\n",
    "        out_minus_y_b= out_minus_y_w\n",
    "        \n",
    "        for i in range(15):\n",
    "            ext_out_minus_y = cc.EvalAdd(out_minus_y_w, ext_out_minus_y)\n",
    "            # Slide out_minus_y_w to the next position\n",
    "            out_minus_y_w = cc.EvalAtIndex(out_minus_y_w, -1)\n",
    "\n",
    "        intermediate_val=self.cc.EvalMult(enc_x,ext_out_minus_y)\n",
    "        intermediate_val = two_party_bootstrapping(intermediate_val, kp1, kp2, self.cc)\n",
    "        self.cc.RescaleInPlace(intermediate_val)\n",
    "        self._delta_w = self.cc.EvalAdd(self._delta_w,intermediate_val)\n",
    "        self._delta_b= self.cc.EvalAdd(self._delta_b,out_minus_y_b)\n",
    "        self._count += 1\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        \n",
    "        # Step 1: Multiply `self.weight_enc` by 0.05\n",
    "        term1 = self.cc.EvalMult(self.weight_enc, self.cc.MakeCKKSPackedPlaintext([0.05] * 15))\n",
    "        self.cc.RescaleInPlace(term1)\n",
    "        # Step 2: Multiply `_delta_w` by (1 / _count)\n",
    "        term2 = self.cc.EvalMult(self._delta_w, self.cc.MakeCKKSPackedPlaintext([1 / self._count] * 15))\n",
    "        self.cc.RescaleInPlace(term2)\n",
    "        # Step 3: Add term1 and term2, then bootstrap to manage noise\n",
    "        intermediate_sum = self.cc.EvalAdd(term1, term2)\n",
    "        \n",
    "        # Step 4: Update `self.weight_enc`\n",
    "        self.weight_enc = self.cc.EvalSub(self.weight_enc, intermediate_sum)\n",
    "        self.weight_enc = two_party_bootstrapping(self.weight_enc, kp1, kp2, cc) \n",
    "    \n",
    "        # Update bias with correct scaling\n",
    "        delta_b_scaled = self.cc.EvalMult(self._delta_b, self.cc.MakeCKKSPackedPlaintext([1 / self._count]))\n",
    "        delta_b_scaled = two_party_bootstrapping(delta_b_scaled, kp1, kp2, cc)\n",
    "        self.bias_enc = self.cc.EvalSub(self.bias_enc, delta_b_scaled)\n",
    "\n",
    "        # Reset gradient accumulators and count\n",
    "        self._delta_w = self.zero9\n",
    "        self._delta_b = self.zero1\n",
    "        self._count = 0\n",
    "    \n",
    "        # Print final decrypted weights and biases\n",
    "        print(\"Updated Weights:\", decrypt_input(cc, kp1.secretKey, kp2.secretKey, self.weight_enc, 15))\n",
    "        print(\"Updated Bias:\", decrypt_input(cc, kp1.secretKey, kp2.secretKey, self.bias_enc, 1))\n",
    "\n",
    "    \n",
    "    def sigmoid(self, enc_x):\n",
    "        # Polynomial coefficients: [0.5, 0.197, 0, -0.004]\n",
    "        coeff_0 = self.cc.MakeCKKSPackedPlaintext([0.5])  # Constant term\n",
    "        coeff_1 = self.cc.MakeCKKSPackedPlaintext([0.197])  # Linear term\n",
    "        coeff_3 = self.cc.MakeCKKSPackedPlaintext([-0.004])  # Cubic term\n",
    "    \n",
    "        # Evaluate cubic term: -0.004 * x^3\n",
    "        enc_x_squared = self.cc.EvalMult(enc_x, enc_x)  # x^2\n",
    "        self.cc.RescaleInPlace(enc_x_squared)\n",
    "        enc_x_cubed = self.cc.EvalMult(enc_x_squared, enc_x)  # x^3\n",
    "        self.cc.RescaleInPlace(enc_x_squared)\n",
    "        term_cubic = self.cc.EvalMult(enc_x_cubed, coeff_3)  # -0.004 * x^3\n",
    "        self.cc.RescaleInPlace(enc_x_squared)\n",
    "        # Evaluate linear term: 0.197 * x\n",
    "        term_linear = self.cc.EvalMult(enc_x, coeff_1)  # 0.197 * x\n",
    "        self.cc.RescaleInPlace(term_linear)\n",
    "        #term_linear = two_party_bootstrapping(term_linear, kp1, kp2, cc)\n",
    "        # Sum all terms: 0.5 + 0.197 * x - 0.004 * x^3\n",
    "        enc_out = self.cc.EvalAdd(term_cubic, term_linear)\n",
    "    \n",
    "        enc_out = self.cc.EvalAdd(enc_out, coeff_0)  # Final sigmoid approximation\n",
    "        self.cc.RescaleInPlace(enc_out)\n",
    "        #print(\"Sigmoid:\", decrypt_input(cc, kp1.secretKey, kp2.secretKey, enc_out, 1))\n",
    "    \n",
    "        return enc_out\n",
    "\n",
    "    \n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "        # evaluate accuracy of the model on\n",
    "        # the plain (x_test, y_test) dataset\n",
    "        w = torch.tensor(self.weight)\n",
    "        b = torch.tensor(self.bias)\n",
    "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
    "        correct = torch.abs(y_test - out) < 0.5\n",
    "        return correct.float().mean()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1056e0-7cde-4ad9-931c-bdef89364c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "# Encrypt and bootstrap X train\n",
    "enc_x_train_initial = [\n",
    "    encrypt_input(cc, kp2.publicKey, x.tolist())\n",
    "    for x in tqdm(x_train, desc=\"Encrypting and Bootstrapping X train\")\n",
    "]\n",
    "\n",
    "# Encrypt and bootstrap Y train\n",
    "enc_y_train_initial = [\n",
    "    encrypt_input(cc, kp2.publicKey, y.tolist())\n",
    "    for y in tqdm(y_train, desc=\"Encrypting and Bootstrapping Y train\")\n",
    "]\n",
    "\n",
    "t_end = time.time()\n",
    "total_time = t_end - t_start\n",
    "print(f\"Encryption and bootstrapping of the train-set took {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d9ad4-dedd-41c2-a41b-feee8590ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "eelr2 = EncryptedLR2(cc, kp2.publicKey, LR(n_features))\n",
    "\n",
    "# List to track time per epoch\n",
    "times = []\n",
    "t_start = time.time()\n",
    "\n",
    "# Loop through the epochs with a progress bar\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    enc_x_train = enc_x_train_initial\n",
    "    enc_y_train = enc_y_train_initial\n",
    "    \n",
    "    # Use tqdm to add a loading bar to the inner loop\n",
    "    for i, (enc_x, enc_y) in enumerate(tqdm(zip(enc_x_train, enc_y_train), total=len(enc_x_train), desc=f\"Training epoch {epoch+1}\")):\n",
    "        \n",
    "        # Perform forward and backward pass\n",
    "        enc_out = eelr2.forward(enc_x)\n",
    "        eelr2.backward(enc_x, enc_out, enc_y)\n",
    "\n",
    "    # Update model parameters after processing the entire dataset\n",
    "    eelr2.update_parameters()\n",
    "t_end = time.time()\n",
    "total_time = t_end - t_start\n",
    "print(f\"\\nTraining of train-set took {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc44fd7-97af-439d-a428-ebcfcc3c0bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypted_evaluation2(model, enc_x_test, y_test):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    correct = 0\n",
    "    outputs = []  # List to store the outputs for plotting\n",
    "    \n",
    "    for enc_x, y in zip(enc_x_test, y_test):\n",
    "        # Encrypted evaluation\n",
    "        enc_out = model(enc_x)\n",
    "        \n",
    "        # Decrypt the encrypted output\n",
    "        out = decrypt_input(cc, kp1.secretKey, kp2.secretKey, enc_out, 1)\n",
    "        out = torch.tensor(out)\n",
    "        \n",
    "        # Apply threshold to make binary prediction\n",
    "        predicted_label = 1 if out >= 0.5 else 0\n",
    "        \n",
    "        # Store the output for plotting (before thresholding)\n",
    "        outputs.append(out.item())\n",
    "        \n",
    "        # Check if the binary prediction matches the true label\n",
    "        if predicted_label == y.item():  # Ensure both are scalars\n",
    "            correct += 1\n",
    "    \n",
    "    t_end = time.time()\n",
    "    total_time = t_end - t_start\n",
    "    print(f\"Evaluated test set of {len(enc_x_test)} entries in {total_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {correct}/{len(enc_x_test)} = {correct / len(enc_x_test)}\")\n",
    "    \n",
    "    # Plot the distribution of model outputs before thresholding\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(outputs, bins=30, color='blue', edgecolor='black')\n",
    "    plt.xlabel(\"Output Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Model Outputs (Before Thresholding)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return correct / len(enc_x_test)\n",
    "\n",
    "encrypted_accuracy = encrypted_evaluation2(eelr2, enc_x_test, y_test)\n",
    "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
